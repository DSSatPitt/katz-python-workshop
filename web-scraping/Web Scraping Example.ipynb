{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Livingsocial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this exercise is to demonstrate some basic web scraping practices using the python programming language. To assist with this exercise we are going to use two 3rd party libraries: An HTTP library called [Requests](http://docs.python-requests.org/en/master/) and a web scraping library called [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/) ([documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import 3rd party libraries for fetching and parsing HTML documents \n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will scrape search results from [Livingsocial](https://www.livingsocial.com/browse/cities/49/searches?utf8=%E2%9C%93&city_search_id=49&country_search_id=1&query=&city_name=Pittsburgh), specifically we are interested in collecting all of information about deals in Pittsburgh in a tabular format.\n",
    "\n",
    "- The base URL is: https://www.livingsocial.com/browse/cities/49/searches?utf8=%E2%9C%93&city_search_id=49&country_search_id=1&query=&city_name=Pittsburgh\n",
    "\n",
    "\n",
    "Why are we scraping Livingsocial?\n",
    "\n",
    "http://monocle.livingsocial.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put the base URL for the web scrape into a variable called \"urly\"\n",
    "entrypoint = \"https://www.livingsocial.com/browse/cities/49/searches?utf8=%E2%9C%93&city_search_id=49&country_search_id=1&query=&city_name=Pittsburgh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fetch the web page containing the Livingsocial deals\n",
    "response = requests.get(entrypoint) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parse the HTML document with Beautiful Soup \n",
    "search_results_page = BeautifulSoup(response.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we have *fetched* and *parsed* the HTML document we can *extract* data.\n",
    "\n",
    "What data do we want to extract? How about a list of all the events!\n",
    "\n",
    "Lets do an *inspect element* on the [listings page](https://www.livingsocial.com/browse/cities/49/searches?utf8=%E2%9C%93&city_search_id=49&country_search_id=1&query=&city_name=Pittsburgh) and see what the HTML structure looks like.\n",
    "\n",
    "![The Livingsocial deals page for Pittsburgh](livingsocial-listings.png)\n",
    "\n",
    "If you look carefully you can see the tag for each deal:\n",
    "`<li dealid=\"1558890\" class=\"deal-tile facet-active search-result multiple-price-points\" data-ga-data=\"\" itemscope=\"\" itemtype=\"http://schema.org/Offer\">` \n",
    "\n",
    "indentifies each row in the list of deals. We can use that to select only the information we want from the rest of the page.\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deals = search_results_page.findAll(\"li\", \"deal-tile\")\n",
    "len(deals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we've extracted 20 deals from the first page of the search results, now we need to extract the relevant information from the HTML structure. Here is what one of those elements looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(deals[0].prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can use Beautiful Soup's `find()` function to extract specific pieces of information from this HTML structure. For more infomation about the find function, see the [Beautiful Soup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find). Then, once we have the HTML tag of interest we can get the data of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deal = deals[0]\n",
    "name = deal.find(\"h2\", itemprop=\"name\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# oops we just want the text content, not the whole element\n",
    "print(name.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to write some code that extracts all of the various bits of information from the HTML structure for each of the deals. Looking at the HTML we can see the name, seller, a description, a location, the URL for that specific deal, a price and something called the strikethrough price (to show the savings I guess). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deal = deals[0]\n",
    "deal_id = deal['dealid']\n",
    "name = deal.find(\"h2\", itemprop=\"name\")\n",
    "seller = deal.find(\"h3\", itemprop=\"seller\")\n",
    "description = deal.find(\"p\", \"description\")\n",
    "location = deal.find(\"p\", \"location\")\n",
    "url = deal.find(\"a\", \"search-wrapper\")\n",
    "price = deal.find(\"div\", \"deal-price\")\n",
    "strikethrough_price = deal.find(\"div\", \"deal-strikethrough-price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(deal_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(name.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(seller.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(description.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(location.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(url['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(price.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(strikethrough_price.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I want to show you want my screen looks like:\n",
    "\n",
    "![The process of webscraping](desktop-view.png)\n",
    "\n",
    "Great! Now that we know how scrape the information from the page, it is time to assemble a \"spider\" that can \"crawl\" through multiple search pages.\n",
    "\n",
    "We've currently scraped 20 deals, but we know by visiting the search page that there are a lot more. We need some code to automatically go to the next page of search results, scrape the deals listings, and repeat. \n",
    "\n",
    "We need to find the URL for the next page and then repeat the scraping process.\n",
    "\n",
    "![HTML for the next button](next-button.png)\n",
    "\n",
    "Looking at the HTML structure I can see it is very easy to find the next button because it has the CSS class `next_page`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "next_button = search_results_page.find(\"a\", \"next_page\")\n",
    "\n",
    "print(next_button['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Sweet! This is all the information I need to build spider/crawler/scraper to automate the process.\n",
    "\n",
    "In the cells below we can assemble the code from the exploratory analysis to automate the web scraping process. \n",
    "The first cell below defines a function for extracting data from the HTML structure of a deal. The second cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_deal_data(deal):\n",
    "    \"\"\"This function takes the raw deal HTML and \n",
    "    extracts eight data points into a python dictionary.\"\"\"\n",
    "\n",
    "    data = {}\n",
    "    try:\n",
    "        data['id'] = deal['dealid'] \n",
    "    except:\n",
    "        data['id'] = \"\"\n",
    "    try:\n",
    "        data['name'] = deal.find(\"h2\", itemprop=\"name\").text\n",
    "    except:\n",
    "        data['name'] = \"\"\n",
    "    try:\n",
    "        data['seller'] = deal.find(\"h3\", itemprop=\"seller\").text \n",
    "    except:\n",
    "        data['seller'] = \"\"\n",
    "    try:\n",
    "        data['description'] = deal.find(\"p\", \"description\").text\n",
    "    except:\n",
    "        data['description'] = \"\"\n",
    "    try:\n",
    "        data['location'] = deal.find(\"p\", \"location\").text\n",
    "    except:\n",
    "        data['location'] = \"\"\n",
    "    try:\n",
    "        data['url'] = deal.find(\"a\", \"search-wrapper\")['href']\n",
    "    except:\n",
    "        data['url'] = \"\"\n",
    "    try:\n",
    "        data['price'] = deal.find(\"div\", \"deal-price\").text\n",
    "    except:\n",
    "        data['price'] = \"\"\n",
    "    try:\n",
    "        data['strikethrough-price'] = deal.find(\"div\", \"deal-strikethrough-price\").text\n",
    "    except:\n",
    "        data['strikethrough-price'] = \"\"\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_deal_data(deal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set some needed variables \n",
    "base = \"https://www.livingsocial.com\"\n",
    "url = \"https://www.livingsocial.com/browse/cities/49/searches?utf8=%E2%9C%93&city_search_id=49&country_search_id=1&query=&city_name=Pittsburgh\"\n",
    "\n",
    "# create a global container\n",
    "all_deals = []\n",
    "\n",
    "# we are going to loop as long as this variable is true\n",
    "crawl = True\n",
    "\n",
    "print(\"Starting crawl.\")\n",
    "\n",
    "while crawl:\n",
    "    \n",
    "    # fetch the page, parse, and get the deals listing\n",
    "    response = requests.get(url)\n",
    "    search_results_page = BeautifulSoup(response.content, 'html.parser')\n",
    "    raw_deals = search_results_page.findAll(\"li\", \"deal-tile\")\n",
    "    \n",
    "    # save the results to a global container\n",
    "    extracted_deals = [extract_deal_data(deal) for deal in raw_deals]\n",
    "    all_deals.extend(extracted_deals)\n",
    "    \n",
    "    # print periodic crawl updates\n",
    "    if len(all_deals) % 500 == 0:\n",
    "        print(\"Collected %d results so far\" % len(all_deals))\n",
    "    \n",
    "    # extract the Next button HTML element\n",
    "    next_button = search_results_page.find(\"a\", \"next_page\")\n",
    "    \n",
    "    # if the CSS class contains disabled, then we've readched the end.\n",
    "    if 'disabled' in next_button['class']:\n",
    "        print(\"Reached the end of the search results. Found %s deals.\" % len(all_deals))\n",
    "        \n",
    "        # setting the crawl variable to false to break the while loop\n",
    "        crawl = False\n",
    "        break\n",
    "    # set the next url to the contents of the next button\n",
    "    url = base + next_button['href']\n",
    "\n",
    "print(\"Crawl completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(all_deals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inspect the contents of the first deal\n",
    "all_deals[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_data = DataFrame(all_deals)\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_data.to_csv(\"scraped-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
